{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import sub\n",
    "from time import time\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras_tqdm import TQDMNotebookCallback as ktqdm\n",
    "from keras.utils import normalize\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.initializers import RandomUniform, RandomNormal\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from tensorflow.nn import relu, softmax\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/paologalligit/nn-thesis-crossvalidation/c72c1ca76b564a5dbf9569998912411c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"CndJ3YmXyZcxmsV8EccJEuu9C\",\n",
    "                        project_name=\"NN_Thesis_crossValidation\", workspace=\"paologalligit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    print(\"{:<15} {:<15} {:<10}\".format('NeuralNetwork','MEAN','STD'))\n",
    "    for k, v in d.items():\n",
    "        print(\"{:<15} {:<15} {:<10}\".format(k, round(v['MEAN'], 4), round(v['STD'], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fifa19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0', 'ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Preferred Foot',\n",
    "                 'Body Type', 'Jersey Number', 'Joined', 'Loaned From', 'Contract Valid Until'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obiettivo: predire valore dei giocatori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing: convertire value, wage e release clause da string a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs=[\"Release Clause\", \"Value\", \"Wage\"]\n",
    "\n",
    "for cur in curs:\n",
    "    def curr2val(x):\n",
    "        x = str(x).replace('â‚¬', '')\n",
    "        if 'K' in x: x = float(str(x).replace('K', '')) * 1000\n",
    "        else: x = float(str(x).replace('M', '')) * 1000000\n",
    "        return x\n",
    "    df[cur] = df[cur].apply(curr2val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individuare eventuali outlier nella colonna value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier(data, threshold = 3):\n",
    "    outliers=[]\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    for y in data:\n",
    "        score= (y - mean) / std \n",
    "        if np.abs(score) > threshold:\n",
    "            outliers.append(y)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_out = min(detect_outlier(df['Value'], threshold = 2))\n",
    "\n",
    "df = df[df['Value'] < min_out] \n",
    "df = df[df['Value'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversione in interi per le altre label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"LS\", \"ST\", \"RS\", \"LW\", \"LF\", \"CF\", \"RF\", \"RW\",\"LAM\", \"CAM\", \"RAM\", \"LM\", \"LCM\", \"CM\", \"RCM\", \"RM\", \"LWB\", \"LDM\",\"CDM\", \"RDM\", \"RWB\", \"LB\", \"LCB\", \"CB\", \"RCB\", \"RB\"]\n",
    "for col in cols:\n",
    "    df[col] = df[col].str[:-2]\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Height'] = df['Height'].str.replace(\"'\",'.')\n",
    "df['Height'] = df['Height'].astype(float)\n",
    "\n",
    "df['Weight'] = df['Weight'].str[:-3]\n",
    "df['Weight'] = df['Weight'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo correlazione tra i valori per scegliere colonne significative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "\n",
    "# fig = plt.figure(figsize=(50,20))\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(df_corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# ticks = np.arange(0,len(df_corr.columns),1)\n",
    "# ax.set_xticks(ticks)\n",
    "# ax.set_xticklabels(df_corr.columns)\n",
    "# plt.xticks(rotation=90)\n",
    "# ax.set_yticks(ticks)\n",
    "# ax.set_yticklabels(df_corr.columns)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Value</th>\n",
       "      <th>Wage</th>\n",
       "      <th>LCM</th>\n",
       "      <th>CM</th>\n",
       "      <th>RCM</th>\n",
       "      <th>Reactions</th>\n",
       "      <th>Release Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.0</td>\n",
       "      <td>7400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>9000000.0</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>9000000.0</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>17100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>4200000.0</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>13000000.0</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>24700000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Overall  Potential       Value     Wage   LCM    CM   RCM  Reactions  \\\n",
       "41        88         88   4000000.0  77000.0   NaN   NaN   NaN       79.0   \n",
       "102       85         85   9000000.0  38000.0  70.0  70.0  70.0       85.0   \n",
       "108       85         85   9000000.0  57000.0  63.0  63.0  63.0       83.0   \n",
       "152       84         84   4200000.0  95000.0  63.0  63.0  63.0       80.0   \n",
       "201       83         83  13000000.0  70000.0   NaN   NaN   NaN       78.0   \n",
       "\n",
       "     Release Clause  \n",
       "41        7400000.0  \n",
       "102      15300000.0  \n",
       "108      17100000.0  \n",
       "152       6900000.0  \n",
       "201      24700000.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for label in df_corr:\n",
    "#     if df_corr['Value'][label] < 0 or df_corr['Value'][label] > 0.5: labels.append(label)\n",
    "    if df_corr['Value'][label] > 0.55: labels.append(label)\n",
    "        \n",
    "df_flt = df[labels]        \n",
    "df_flt.head()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mescolo le righe del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flt = df_flt.sample(frac=1)\n",
    "\n",
    "train_slice = int(len(df_flt) * 0.8)\n",
    "\n",
    "train = df_flt[:train_slice]\n",
    "test = df_flt[train_slice:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.loc[:, ['Value']]\n",
    "X_train = train.drop(columns='Value')\n",
    "\n",
    "y_test = test.loc[:, ['Value']]\n",
    "X_test = test.drop(columns='Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sostiuisco eventuali valori nan con la media della colonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X_train)\n",
    "X_full = imputer.transform(X_train)\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(y_train)\n",
    "y_full = imputer.transform(y_train)\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X_test)\n",
    "X_test_full = imputer.transform(X_test)\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(y_test)\n",
    "y_test_full = imputer.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalo i valori, sia per i caratteri che per il target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = RobustScaler()\n",
    "# scaler = scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "\n",
    "# X_train_scaled = preprocessing.scale(X_train)\n",
    "scaler = StandardScaler().fit(X_full)\n",
    "X_scaled = scaler.transform(X_full)\n",
    "\n",
    "scaler_train = StandardScaler().fit(y_full)\n",
    "y_scaled = scaler_train.transform(y_full) \n",
    "# X_train_scaled, X_test_scaled\n",
    "\n",
    "scaler = StandardScaler().fit(X_test_full)\n",
    "X_test_scaled = scaler.transform(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_test, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_test-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_test - K.mean(y_test) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_board = TensorBoard(log_dir='value_predictions_v3/{}'.format('kfold_' + str(time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_error(prediction, target):\n",
    "    res = 0\n",
    "    tot = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i][0] != 0 and not np.isnan(prediction[i][0]):\n",
    "#             print(prediction[i][0], target[i][0])\n",
    "            res += abs(prediction[i][0] - target[i][0])\n",
    "            tot += 1\n",
    "#     print(tot, len(target), len(prediction))\n",
    "    return round(res / tot, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "kfold = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H1(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(n_per_layer, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "#     model.add(Dropout(0.1))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "#     model.add(Dense(128, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "#     model.add(Dropout(0.1))\n",
    "    \n",
    "    # Adding the second hidden layer\n",
    "#     model.add(Dense(64, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "#     model.add(LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/paologio/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H1(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H1(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H1(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H1(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H1(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "WARNING:tensorflow:From /home/paologio/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "R2 of round:  0.9672\n",
      "12535/12535 [==============================] - 0s 14us/step\n",
      "loss: 0.0711, acc: 96.0324%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9802\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0610, acc: 96.6972%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9783\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0590, acc: 96.7161%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9799\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0588, acc: 97.1320%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9799\n",
      "12535/12535 [==============================] - 0s 10us/step\n",
      "loss: 0.0574, acc: 97.4592%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.978\n",
      "12535/12535 [==============================] - 0s 14us/step\n",
      "loss: 0.0548, acc: 97.5795%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9713\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0563, acc: 97.8065%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9772\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0546, acc: 97.9421%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9876\n",
      "12536/12536 [==============================] - 0s 11us/step\n",
      "loss: 0.0548, acc: 97.7756%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9806\n",
      "12536/12536 [==============================] - 0s 11us/step\n",
      "loss: 0.0534, acc: 97.8407%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9801\n",
      "12535/12535 [==============================] - 0s 15us/step\n",
      "loss: 0.0597, acc: 96.8329%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9846\n",
      "12535/12535 [==============================] - 0s 16us/step\n",
      "loss: 0.0557, acc: 97.3639%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9837\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0549, acc: 97.6193%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9816\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0542, acc: 97.8308%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9778\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0541, acc: 98.0354%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9802\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0527, acc: 97.9806%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9745\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0536, acc: 98.0638%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9808\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0506, acc: 98.1879%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9877\n",
      "12536/12536 [==============================] - 0s 11us/step\n",
      "loss: 0.0517, acc: 98.1503%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9832\n",
      "12536/12536 [==============================] - 0s 11us/step\n",
      "loss: 0.0535, acc: 98.1120%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9751\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0583, acc: 97.3355%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9865\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0525, acc: 97.9949%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9861\n",
      "12535/12535 [==============================] - 0s 14us/step\n",
      "loss: 0.0517, acc: 98.1373%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9835\n",
      "12535/12535 [==============================] - 0s 11us/step\n",
      "loss: 0.0502, acc: 98.3559%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9808\n",
      "12535/12535 [==============================] - 0s 12us/step\n",
      "loss: 0.0507, acc: 98.4401%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9818\n",
      "12535/12535 [==============================] - 0s 12us/step\n",
      "loss: 0.0501, acc: 98.2791%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.98\n",
      "12535/12535 [==============================] - 0s 12us/step\n",
      "loss: 0.0482, acc: 98.3942%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9831\n",
      "12535/12535 [==============================] - 0s 12us/step\n",
      "loss: 0.0512, acc: 98.3962%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9892\n",
      "12536/12536 [==============================] - 0s 13us/step\n",
      "loss: 0.0493, acc: 98.3134%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9853\n",
      "12536/12536 [==============================] - 0s 11us/step\n",
      "loss: 0.0475, acc: 98.4558%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9812\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0557, acc: 97.7495%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9849\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0528, acc: 98.0142%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9857\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0503, acc: 98.2684%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9819\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0510, acc: 98.1797%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.978\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0514, acc: 98.2637%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9829\n",
      "12535/12535 [==============================] - 0s 14us/step\n",
      "loss: 0.0479, acc: 98.3504%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9792\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0481, acc: 98.3690%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9815\n",
      "12535/12535 [==============================] - 0s 13us/step\n",
      "loss: 0.0493, acc: 98.3989%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12536/12536 [==============================] - 0s 15us/step\n",
      "loss: 0.0471, acc: 98.4242%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9854\n",
      "12536/12536 [==============================] - 0s 22us/step\n",
      "loss: 0.0516, acc: 98.1462%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9802\n",
      "12535/12535 [==============================] - 0s 20us/step\n",
      "loss: 0.0529, acc: 97.9431%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9866\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0532, acc: 98.1704%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9851\n",
      "12535/12535 [==============================] - 0s 15us/step\n",
      "loss: 0.0535, acc: 98.1728%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9812\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0492, acc: 98.3682%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9793\n",
      "12535/12535 [==============================] - 0s 14us/step\n",
      "loss: 0.0494, acc: 98.4623%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.984\n",
      "12535/12535 [==============================] - 0s 15us/step\n",
      "loss: 0.0471, acc: 98.3437%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9801\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0493, acc: 98.3750%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9835\n",
      "12535/12535 [==============================] - 0s 15us/step\n",
      "loss: 0.0488, acc: 98.5722%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9895\n",
      "12536/12536 [==============================] - 0s 14us/step\n",
      "loss: 0.0482, acc: 98.4772%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.988\n",
      "12536/12536 [==============================] - 0s 14us/step\n",
      "loss: 0.0471, acc: 98.4554%\n",
      "CPU times: user 19min 1s, sys: 1min 30s, total: 20min 31s\n",
      "Wall time: 11min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           97.2981         60.2518   \n",
      "nn_128          97.8177         41.0398   \n",
      "nn_256          98.2102         32.1716   \n",
      "nn_512          98.2164         19.6934   \n",
      "nn_1024         98.334          17.8023   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    # y_test_wrong = y_scaled[:3483]\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    # scaler_test = StandardScaler().fit(y_test)\n",
    "    # res = scaler_test.inverse_transform(res)\n",
    "    # y_test_wrong = scaler_test.inverse_transform(y_test_wrong)\n",
    "    # print(res)\n",
    "\n",
    "    # mean_abs_error(res, y_test_wrong), max(y_test_wrong), round(np.mean(y_test_wrong), 2), round(r2_score(y_test_wrong, res), 4)\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            144965.39       13500000.0      1621035.03      97.5739        \n",
      "t_128           147991.03       13500000.0      1621035.03      97.6184        \n",
      "t_256           139333.52       13500000.0      1621035.03      97.849         \n",
      "t_512           150002.13       13500000.0      1621035.03      97.6027        \n",
      "t_1024          143280.23       13500000.0      1621035.03      97.7151        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2']))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RISULTATO: la rete con un solo strato sembra aver ottenuto risultati migliori con 512 neuroni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con due strati nascosti (H1 - 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H2(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(512, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "    model.add(Dense(n_per_layer, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H2(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H2(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H2(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H2(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H2(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9877\n",
      "12535/12535 [==============================] - 0s 24us/step\n",
      "loss: 0.0471, acc: 98.1418%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9869\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0400, acc: 98.6657%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9886\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0378, acc: 98.8437%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9895\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0378, acc: 99.0533%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9872\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0349, acc: 99.2445%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9926\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0320, acc: 99.3800%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9915\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0279, acc: 99.4929%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9922\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0279, acc: 99.5332%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12536/12536 [==============================] - 0s 20us/step\n",
      "loss: 0.0271, acc: 99.5628%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9947\n",
      "12536/12536 [==============================] - 0s 21us/step\n",
      "loss: 0.0255, acc: 99.6163%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9883\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0454, acc: 98.1352%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9889\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0380, acc: 98.9358%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9893\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0357, acc: 99.1125%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9898\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0328, acc: 99.3035%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9874\n",
      "12535/12535 [==============================] - 0s 23us/step\n",
      "loss: 0.0317, acc: 99.3797%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0292, acc: 99.4411%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.992\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0283, acc: 99.5287%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0288, acc: 99.5168%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12536/12536 [==============================] - 0s 19us/step\n",
      "loss: 0.0267, acc: 99.5738%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9949\n",
      "12536/12536 [==============================] - 0s 18us/step\n",
      "loss: 0.0245, acc: 99.6355%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9862\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0445, acc: 98.4718%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9883\n",
      "12535/12535 [==============================] - 0s 23us/step\n",
      "loss: 0.0386, acc: 98.7195%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9887\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0346, acc: 99.1383%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9914\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0318, acc: 99.3797%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9848\n",
      "12535/12535 [==============================] - 0s 27us/step\n",
      "loss: 0.0320, acc: 99.3883%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.995\n",
      "12535/12535 [==============================] - 0s 24us/step\n",
      "loss: 0.0276, acc: 99.4443%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.994\n",
      "12535/12535 [==============================] - 0s 23us/step\n",
      "loss: 0.0276, acc: 99.5205%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0241, acc: 99.6358%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9962\n",
      "12536/12536 [==============================] - 0s 23us/step\n",
      "loss: 0.0230, acc: 99.6624%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9962\n",
      "12536/12536 [==============================] - 0s 23us/step\n",
      "loss: 0.0232, acc: 99.6679%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9889\n",
      "12535/12535 [==============================] - 1s 46us/step\n",
      "loss: 0.0433, acc: 98.5085%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9886\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0386, acc: 98.7148%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.988\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0345, acc: 99.0192%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9909\n",
      "12535/12535 [==============================] - 1s 53us/step\n",
      "loss: 0.0311, acc: 99.3508%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9866\n",
      "12535/12535 [==============================] - 0s 35us/step\n",
      "loss: 0.0284, acc: 99.4854%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9942\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0275, acc: 99.4611%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 1s 48us/step\n",
      "loss: 0.0260, acc: 99.5623%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.994\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0240, acc: 99.6112%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9947\n",
      "12536/12536 [==============================] - 0s 35us/step\n",
      "loss: 0.0229, acc: 99.6802%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9949\n",
      "12536/12536 [==============================] - 0s 33us/step\n",
      "loss: 0.0217, acc: 99.6940%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9889\n",
      "12535/12535 [==============================] - 1s 60us/step\n",
      "loss: 0.0446, acc: 98.1189%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9869\n",
      "12535/12535 [==============================] - 1s 51us/step\n",
      "loss: 0.0385, acc: 99.0347%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9872\n",
      "12535/12535 [==============================] - 1s 54us/step\n",
      "loss: 0.0339, acc: 99.1761%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9918\n",
      "12535/12535 [==============================] - 1s 61us/step\n",
      "loss: 0.0317, acc: 99.3190%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.987\n",
      "12535/12535 [==============================] - 1s 49us/step\n",
      "loss: 0.0309, acc: 99.5061%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9953\n",
      "12535/12535 [==============================] - 1s 50us/step\n",
      "loss: 0.0278, acc: 99.5255%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 1s 69us/step\n",
      "loss: 0.0239, acc: 99.6317%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9941\n",
      "12535/12535 [==============================] - 1s 69us/step\n",
      "loss: 0.0237, acc: 99.6287%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9957\n",
      "12536/12536 [==============================] - 1s 59us/step\n",
      "loss: 0.0223, acc: 99.6966%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9967\n",
      "12536/12536 [==============================] - 1s 76us/step\n",
      "loss: 0.0217, acc: 99.7032%\n",
      "CPU times: user 2h 12min 37s, sys: 4min 50s, total: 2h 17min 27s\n",
      "Wall time: 48min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           99.1534         45.4775   \n",
      "nn_128          99.2563         42.6444   \n",
      "nn_256          99.3028         38.8859   \n",
      "nn_512          99.3088         39.6924   \n",
      "nn_1024         99.3341         45.8342   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            136107.72       13500000.0      1621035.03      98.2002        \n",
      "t_128           135544.17       13500000.0      1621035.03      98.257         \n",
      "t_256           133655.11       13500000.0      1621035.03      98.2545        \n",
      "t_512           138729.92       13500000.0      1621035.03      98.0812        \n",
      "t_1024          136577.55       13500000.0      1621035.03      98.3437        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con due strati nascosti (H1 - 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H2(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(256, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "    model.add(Dense(n_per_layer, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H2(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H2(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H2(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H2(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H2(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9875\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0459, acc: 98.2851%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9871\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0451, acc: 98.3314%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9897\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0388, acc: 98.9928%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9916\n",
      "12535/12535 [==============================] - 0s 20us/step\n",
      "loss: 0.0354, acc: 99.2586%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9877\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0348, acc: 99.2624%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9934\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0334, acc: 99.3696%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9927\n",
      "12535/12535 [==============================] - 0s 17us/step\n",
      "loss: 0.0308, acc: 99.3663%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9913\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0298, acc: 99.4413%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9946\n",
      "12536/12536 [==============================] - 0s 18us/step\n",
      "loss: 0.0296, acc: 99.4622%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9925\n",
      "12536/12536 [==============================] - 0s 17us/step\n",
      "loss: 0.0293, acc: 99.5001%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9861\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0447, acc: 98.2748%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9906\n",
      "12535/12535 [==============================] - 0s 20us/step\n",
      "loss: 0.0401, acc: 98.8525%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9904\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0351, acc: 99.1597%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9902\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0347, acc: 99.2665%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9909\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0306, acc: 99.4123%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9947\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0286, acc: 99.4959%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.993\n",
      "12535/12535 [==============================] - 0s 20us/step\n",
      "loss: 0.0276, acc: 99.5597%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9925\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0265, acc: 99.5710%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9946\n",
      "12536/12536 [==============================] - 0s 23us/step\n",
      "loss: 0.0259, acc: 99.6077%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.995\n",
      "12536/12536 [==============================] - 0s 19us/step\n",
      "loss: 0.0245, acc: 99.6470%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 1s 43us/step\n",
      "loss: 0.0423, acc: 98.5078%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9879\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0369, acc: 98.8328%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9889\n",
      "12535/12535 [==============================] - 0s 25us/step\n",
      "loss: 0.0342, acc: 99.2081%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9894\n",
      "12535/12535 [==============================] - 0s 29us/step\n",
      "loss: 0.0317, acc: 99.3777%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9892\n",
      "12535/12535 [==============================] - 0s 25us/step\n",
      "loss: 0.0285, acc: 99.4922%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9957\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0271, acc: 99.5083%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9947\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0260, acc: 99.5545%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9928\n",
      "12535/12535 [==============================] - 0s 23us/step\n",
      "loss: 0.0248, acc: 99.5946%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9952\n",
      "12536/12536 [==============================] - 0s 26us/step\n",
      "loss: 0.0252, acc: 99.6070%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9947\n",
      "12536/12536 [==============================] - 0s 23us/step\n",
      "loss: 0.0259, acc: 99.5788%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9883\n",
      "12535/12535 [==============================] - 1s 52us/step\n",
      "loss: 0.0431, acc: 98.2361%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9865\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0393, acc: 98.8569%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9891\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0340, acc: 99.2492%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9896\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0313, acc: 99.3381%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9843\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0329, acc: 99.4102%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.996\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0272, acc: 99.5055%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0256, acc: 99.5648%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9943\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0250, acc: 99.6204%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.995\n",
      "12536/12536 [==============================] - 0s 31us/step\n",
      "loss: 0.0254, acc: 99.6029%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9957\n",
      "12536/12536 [==============================] - 0s 27us/step\n",
      "loss: 0.0241, acc: 99.6394%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9876\n",
      "12535/12535 [==============================] - 1s 60us/step\n",
      "loss: 0.0450, acc: 98.3602%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.986\n",
      "12535/12535 [==============================] - 1s 40us/step\n",
      "loss: 0.0399, acc: 98.9422%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.99\n",
      "12535/12535 [==============================] - 0s 40us/step\n",
      "loss: 0.0320, acc: 99.3444%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9897\n",
      "12535/12535 [==============================] - 1s 45us/step\n",
      "loss: 0.0321, acc: 99.2788%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9884\n",
      "12535/12535 [==============================] - 1s 40us/step\n",
      "loss: 0.0265, acc: 99.5810%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9949\n",
      "12535/12535 [==============================] - 1s 48us/step\n",
      "loss: 0.0261, acc: 99.5571%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12535/12535 [==============================] - 1s 55us/step\n",
      "loss: 0.0233, acc: 99.6562%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12535/12535 [==============================] - 0s 40us/step\n",
      "loss: 0.0234, acc: 99.6465%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9963\n",
      "12536/12536 [==============================] - 1s 51us/step\n",
      "loss: 0.0217, acc: 99.7142%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9962\n",
      "12536/12536 [==============================] - 0s 37us/step\n",
      "loss: 0.0215, acc: 99.7379%\n",
      "CPU times: user 1h 14min 45s, sys: 3min 31s, total: 1h 18min 16s\n",
      "Wall time: 31min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           99.127          43.1505   \n",
      "nn_128          99.2847         40.9494   \n",
      "nn_256          99.3262         35.4593   \n",
      "nn_512          99.3024         42.0276   \n",
      "nn_1024         99.3819         41.2556   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            134708.36       13500000.0      1621035.03      98.1627        \n",
      "t_128           136847.86       13500000.0      1621035.03      98.1763        \n",
      "t_256           137330.89       13500000.0      1621035.03      98.198         \n",
      "t_512           141966.77       13500000.0      1621035.03      98.1409        \n",
      "t_1024          143702.25       13500000.0      1621035.03      97.9681        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con due strati nascosti (H1 - 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H2(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(128, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "    model.add(Dense(n_per_layer, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H2(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H2(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H2(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H2(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H2(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9868\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0489, acc: 98.0550%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9897\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0443, acc: 98.5115%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9877\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0403, acc: 98.6333%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9889\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0392, acc: 98.7825%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9828\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0368, acc: 98.8563%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.992\n",
      "12535/12535 [==============================] - 0s 18us/step\n",
      "loss: 0.0380, acc: 98.9705%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9855\n",
      "12535/12535 [==============================] - 0s 19us/step\n",
      "loss: 0.0353, acc: 99.1555%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9905\n",
      "12535/12535 [==============================] - 0s 22us/step\n",
      "loss: 0.0365, acc: 99.2305%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9925\n",
      "12536/12536 [==============================] - 0s 19us/step\n",
      "loss: 0.0339, acc: 99.2791%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9925\n",
      "12536/12536 [==============================] - 0s 20us/step\n",
      "loss: 0.0312, acc: 99.3664%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9858\n",
      "12535/12535 [==============================] - 1s 43us/step\n",
      "loss: 0.0463, acc: 98.3001%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.987\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0463, acc: 98.5662%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9887\n",
      "12535/12535 [==============================] - 0s 21us/step\n",
      "loss: 0.0363, acc: 98.9532%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9897\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0344, acc: 99.2144%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.986\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0328, acc: 99.1711%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9921\n",
      "12535/12535 [==============================] - 0s 27us/step\n",
      "loss: 0.0340, acc: 99.2885%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9902\n",
      "12535/12535 [==============================] - 0s 25us/step\n",
      "loss: 0.0307, acc: 99.4168%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9913\n",
      "12535/12535 [==============================] - 0s 25us/step\n",
      "loss: 0.0316, acc: 99.4075%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9927\n",
      "12536/12536 [==============================] - 0s 24us/step\n",
      "loss: 0.0298, acc: 99.4861%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9935\n",
      "12536/12536 [==============================] - 0s 35us/step\n",
      "loss: 0.0297, acc: 99.4577%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9831\n",
      "12535/12535 [==============================] - 1s 58us/step\n",
      "loss: 0.0477, acc: 98.2700%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9874\n",
      "12535/12535 [==============================] - 0s 27us/step\n",
      "loss: 0.0412, acc: 98.7646%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9893\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0354, acc: 99.1751%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9901\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0342, acc: 99.2244%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9898\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0311, acc: 99.4029%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9938\n",
      "12535/12535 [==============================] - 0s 27us/step\n",
      "loss: 0.0304, acc: 99.4084%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0297, acc: 99.4498%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9911\n",
      "12535/12535 [==============================] - 0s 29us/step\n",
      "loss: 0.0302, acc: 99.4616%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9948\n",
      "12536/12536 [==============================] - 0s 27us/step\n",
      "loss: 0.0274, acc: 99.5117%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9934\n",
      "12536/12536 [==============================] - 0s 29us/step\n",
      "loss: 0.0270, acc: 99.5578%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9861\n",
      "12535/12535 [==============================] - 1s 58us/step\n",
      "loss: 0.0430, acc: 98.3639%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9874\n",
      "12535/12535 [==============================] - 0s 35us/step\n",
      "loss: 0.0407, acc: 98.6559%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9887\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0335, acc: 99.2630%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9902\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0309, acc: 99.3886%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9861\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0283, acc: 99.5179%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9954\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0249, acc: 99.5386%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0247, acc: 99.5941%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0255, acc: 99.5743%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9948\n",
      "12536/12536 [==============================] - 0s 34us/step\n",
      "loss: 0.0227, acc: 99.6522%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9951\n",
      "12536/12536 [==============================] - 0s 33us/step\n",
      "loss: 0.0257, acc: 99.6192%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9871\n",
      "12535/12535 [==============================] - 1s 67us/step\n",
      "loss: 0.0463, acc: 98.2372%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9887\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0357, acc: 98.9891%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9899\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0354, acc: 99.1602%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9915\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0296, acc: 99.4160%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0316, acc: 99.4093%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9948\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0283, acc: 99.4938%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9936\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0260, acc: 99.5546%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9941\n",
      "12535/12535 [==============================] - 1s 41us/step\n",
      "loss: 0.0239, acc: 99.6215%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.995\n",
      "12536/12536 [==============================] - 1s 61us/step\n",
      "loss: 0.0236, acc: 99.6491%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9959\n",
      "12536/12536 [==============================] - 0s 36us/step\n",
      "loss: 0.0219, acc: 99.6986%\n",
      "CPU times: user 49min 6s, sys: 2min 55s, total: 52min 2s\n",
      "Wall time: 25min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           98.8841         38.6008   \n",
      "nn_128          99.1262         38.2752   \n",
      "nn_256          99.2226         38.6135   \n",
      "nn_512          99.3168         42.3036   \n",
      "nn_1024         99.3229         41.8602   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            138641.74       13500000.0      1621035.03      98.2178        \n",
      "t_128           134634.01       13500000.0      1621035.03      98.2689        \n",
      "t_256           145657.85       13500000.0      1621035.03      98.0459        \n",
      "t_512           143588.99       13500000.0      1621035.03      97.8723        \n",
      "t_1024          144390.69       13500000.0      1621035.03      97.9785        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con tre strati nascosti (H1 - 512, H2 - 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H3(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(512, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "    model.add(Dense(64, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(n_per_layer, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H3(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H3(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H3(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H3(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H3(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.988\n",
      "12535/12535 [==============================] - 1s 59us/step\n",
      "loss: 0.0439, acc: 98.3213%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9881\n",
      "12535/12535 [==============================] - 0s 29us/step\n",
      "loss: 0.0397, acc: 98.9256%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9895\n",
      "12535/12535 [==============================] - 0s 35us/step\n",
      "loss: 0.0341, acc: 99.2632%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9908\n",
      "12535/12535 [==============================] - 0s 26us/step\n",
      "loss: 0.0310, acc: 99.4187%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9879\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0326, acc: 99.4148%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9943\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0296, acc: 99.4815%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9935\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0250, acc: 99.6015%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.993\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0245, acc: 99.6353%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9951\n",
      "12536/12536 [==============================] - 0s 27us/step\n",
      "loss: 0.0234, acc: 99.6634%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9944\n",
      "12536/12536 [==============================] - 0s 28us/step\n",
      "loss: 0.0227, acc: 99.6634%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9856\n",
      "12535/12535 [==============================] - 1s 55us/step\n",
      "loss: 0.0534, acc: 97.7934%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9886\n",
      "12535/12535 [==============================] - 0s 27us/step\n",
      "loss: 0.0373, acc: 98.9909%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9909\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0347, acc: 99.2485%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9916\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0321, acc: 99.3373%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 0s 28us/step\n",
      "loss: 0.0308, acc: 99.3819%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9949\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0303, acc: 99.4159%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0259, acc: 99.5592%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9931\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0271, acc: 99.5372%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9931\n",
      "12536/12536 [==============================] - 0s 39us/step\n",
      "loss: 0.0303, acc: 99.4476%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9952\n",
      "12536/12536 [==============================] - 0s 27us/step\n",
      "loss: 0.0242, acc: 99.6223%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9871\n",
      "12535/12535 [==============================] - 1s 58us/step\n",
      "loss: 0.0453, acc: 98.4882%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9881\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0400, acc: 98.7972%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9899\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0346, acc: 99.1795%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.989\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0319, acc: 99.2391%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9897\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0303, acc: 99.4062%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9939\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0298, acc: 99.4217%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0285, acc: 99.4628%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9928\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0263, acc: 99.5299%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9941\n",
      "12536/12536 [==============================] - 0s 30us/step\n",
      "loss: 0.0249, acc: 99.5876%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9951\n",
      "12536/12536 [==============================] - 0s 30us/step\n",
      "loss: 0.0252, acc: 99.5996%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 1s 63us/step\n",
      "loss: 0.0423, acc: 98.4685%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.988\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0418, acc: 98.9499%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9884\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0390, acc: 99.0519%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9908\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0338, acc: 99.2749%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9907\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0308, acc: 99.3894%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9939\n",
      "12535/12535 [==============================] - 0s 35us/step\n",
      "loss: 0.0299, acc: 99.4027%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9931\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0326, acc: 99.3238%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9914\n",
      "12535/12535 [==============================] - 0s 33us/step\n",
      "loss: 0.0356, acc: 99.2330%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9942\n",
      "12536/12536 [==============================] - 0s 32us/step\n",
      "loss: 0.0272, acc: 99.5094%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9926\n",
      "12536/12536 [==============================] - 0s 33us/step\n",
      "loss: 0.0271, acc: 99.5187%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9874\n",
      "12535/12535 [==============================] - 1s 66us/step\n",
      "loss: 0.0472, acc: 98.3772%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9859\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0399, acc: 98.6216%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9888\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0376, acc: 98.9078%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9903\n",
      "12535/12535 [==============================] - 1s 41us/step\n",
      "loss: 0.0328, acc: 99.0976%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.992\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0289, acc: 99.3146%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.995\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0276, acc: 99.4903%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.992\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0275, acc: 99.4886%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9932\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0261, acc: 99.4864%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9946\n",
      "12536/12536 [==============================] - 0s 37us/step\n",
      "loss: 0.0248, acc: 99.5904%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9954\n",
      "12536/12536 [==============================] - 0s 39us/step\n",
      "loss: 0.0233, acc: 99.6145%\n",
      "CPU times: user 1h 6s, sys: 3min 23s, total: 1h 3min 29s\n",
      "Wall time: 29min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           99.3389         40.1488   \n",
      "nn_128          99.2334         50.935    \n",
      "nn_256          99.2712         34.6346   \n",
      "nn_512          99.2122         30.2396   \n",
      "nn_1024         99.1989         41.1612   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            139719.77       13500000.0      1621035.03      98.0239        \n",
      "t_128           134200.32       13500000.0      1621035.03      98.2085        \n",
      "t_256           132223.74       13500000.0      1621035.03      98.2737        \n",
      "t_512           132525.49       13500000.0      1621035.03      98.3477        \n",
      "t_1024          137201.69       13500000.0      1621035.03      98.2118        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prova con tre strati nascosti (H1 - 256, H2 - 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_H3(neurons, n_per_layer):\n",
    "    model = Sequential()\n",
    "    # Adding the input layer\n",
    "    model.add(Dense(256, input_dim = neurons, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the first hidden layer\n",
    "    model.add(Dense(128, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(n_per_layer, activation='relu'))#, kernel_regularizer=l2(0.001)))#, kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "    \n",
    "    # Adding the output layer\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "nn64 = build_nn_H3(X_scaled.shape[1], 64)\n",
    "nn128 = build_nn_H3(X_scaled.shape[1], 128)\n",
    "nn256 = build_nn_H3(X_scaled.shape[1], 256)\n",
    "nn512 = build_nn_H3(X_scaled.shape[1], 512)\n",
    "nn1024 = build_nn_H3(X_scaled.shape[1], 1024)\n",
    "# opt = SGD(lr=0.01, momentum=0.9)\n",
    "nn64.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn128.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn256.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn512.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "nn1024.compile(optimizer = opt, loss = 'mean_absolute_error', metrics = [coeff_determination])\n",
    "\n",
    "nn_models = [nn64, nn128, nn256, nn512, nn1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "NEURAL NETWORK -->  1\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9863\n",
      "12535/12535 [==============================] - 1s 66us/step\n",
      "loss: 0.0428, acc: 98.6423%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9878\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0365, acc: 99.1050%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9904\n",
      "12535/12535 [==============================] - 0s 32us/step\n",
      "loss: 0.0303, acc: 99.3931%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9896\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0278, acc: 99.5129%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9918\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0271, acc: 99.5775%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9958\n",
      "12535/12535 [==============================] - 0s 30us/step\n",
      "loss: 0.0238, acc: 99.6639%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9931\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0249, acc: 99.6323%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9954\n",
      "12535/12535 [==============================] - 0s 31us/step\n",
      "loss: 0.0212, acc: 99.7330%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9956\n",
      "12536/12536 [==============================] - 0s 31us/step\n",
      "loss: 0.0212, acc: 99.7387%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9973\n",
      "12536/12536 [==============================] - 0s 32us/step\n",
      "loss: 0.0176, acc: 99.8086%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  2\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.988\n",
      "12535/12535 [==============================] - 1s 70us/step\n",
      "loss: 0.0419, acc: 98.5737%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9883\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0352, acc: 99.2055%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9898\n",
      "12535/12535 [==============================] - 0s 35us/step\n",
      "loss: 0.0348, acc: 99.3387%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.991\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0308, acc: 99.4305%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9902\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0339, acc: 99.3305%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9951\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0242, acc: 99.6146%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.995\n",
      "12535/12535 [==============================] - 0s 36us/step\n",
      "loss: 0.0235, acc: 99.6432%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9946\n",
      "12535/12535 [==============================] - 0s 34us/step\n",
      "loss: 0.0248, acc: 99.5980%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9963\n",
      "12536/12536 [==============================] - 0s 34us/step\n",
      "loss: 0.0214, acc: 99.7220%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9959\n",
      "12536/12536 [==============================] - 0s 34us/step\n",
      "loss: 0.0219, acc: 99.6976%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  3\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9864\n",
      "12535/12535 [==============================] - 1s 76us/step\n",
      "loss: 0.0421, acc: 98.8480%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9879\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0364, acc: 99.2163%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9901\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0331, acc: 99.2986%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9915\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0300, acc: 99.4039%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9898\n",
      "12535/12535 [==============================] - 0s 40us/step\n",
      "loss: 0.0289, acc: 99.4866%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9938\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0281, acc: 99.4666%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 0s 38us/step\n",
      "loss: 0.0262, acc: 99.5512%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9942\n",
      "12535/12535 [==============================] - 0s 37us/step\n",
      "loss: 0.0246, acc: 99.5568%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9955\n",
      "12536/12536 [==============================] - 1s 43us/step\n",
      "loss: 0.0216, acc: 99.6805%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9956\n",
      "12536/12536 [==============================] - 0s 36us/step\n",
      "loss: 0.0210, acc: 99.6926%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  4\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9857\n",
      "12535/12535 [==============================] - 1s 78us/step\n",
      "loss: 0.0448, acc: 98.5443%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 1s 44us/step\n",
      "loss: 0.0394, acc: 98.9175%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.991\n",
      "12535/12535 [==============================] - 0s 39us/step\n",
      "loss: 0.0326, acc: 99.2945%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9895\n",
      "12535/12535 [==============================] - 1s 56us/step\n",
      "loss: 0.0313, acc: 99.3501%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9916\n",
      "12535/12535 [==============================] - 1s 58us/step\n",
      "loss: 0.0267, acc: 99.5170%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9954\n",
      "12535/12535 [==============================] - 1s 48us/step\n",
      "loss: 0.0254, acc: 99.5314%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 1s 48us/step\n",
      "loss: 0.0240, acc: 99.6025%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9933\n",
      "12535/12535 [==============================] - 1s 48us/step\n",
      "loss: 0.0265, acc: 99.5845%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9946\n",
      "12536/12536 [==============================] - 1s 49us/step\n",
      "loss: 0.0214, acc: 99.6818%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.9956\n",
      "12536/12536 [==============================] - 1s 51us/step\n",
      "loss: 0.0203, acc: 99.7071%\n",
      "############################################################\n",
      "NEURAL NETWORK -->  5\n",
      "############################################################\n",
      "\n",
      "stage --> 1\n",
      "\n",
      "R2 of round:  0.9871\n",
      "12535/12535 [==============================] - 1s 106us/step\n",
      "loss: 0.0409, acc: 98.6479%\n",
      "\n",
      "stage --> 2\n",
      "\n",
      "R2 of round:  0.9892\n",
      "12535/12535 [==============================] - 1s 56us/step\n",
      "loss: 0.0368, acc: 99.1052%\n",
      "\n",
      "stage --> 3\n",
      "\n",
      "R2 of round:  0.9907\n",
      "12535/12535 [==============================] - 1s 57us/step\n",
      "loss: 0.0320, acc: 99.3540%\n",
      "\n",
      "stage --> 4\n",
      "\n",
      "R2 of round:  0.9924\n",
      "12535/12535 [==============================] - 1s 58us/step\n",
      "loss: 0.0289, acc: 99.4270%\n",
      "\n",
      "stage --> 5\n",
      "\n",
      "R2 of round:  0.9882\n",
      "12535/12535 [==============================] - 1s 56us/step\n",
      "loss: 0.0265, acc: 99.4912%\n",
      "\n",
      "stage --> 6\n",
      "\n",
      "R2 of round:  0.9963\n",
      "12535/12535 [==============================] - 1s 57us/step\n",
      "loss: 0.0243, acc: 99.5454%\n",
      "\n",
      "stage --> 7\n",
      "\n",
      "R2 of round:  0.9937\n",
      "12535/12535 [==============================] - 1s 56us/step\n",
      "loss: 0.0247, acc: 99.5818%\n",
      "\n",
      "stage --> 8\n",
      "\n",
      "R2 of round:  0.9929\n",
      "12535/12535 [==============================] - 1s 60us/step\n",
      "loss: 0.0245, acc: 99.5939%\n",
      "\n",
      "stage --> 9\n",
      "\n",
      "R2 of round:  0.9951\n",
      "12536/12536 [==============================] - 1s 58us/step\n",
      "loss: 0.0233, acc: 99.6433%\n",
      "\n",
      "stage --> 10\n",
      "\n",
      "R2 of round:  0.995\n",
      "12536/12536 [==============================] - 1s 56us/step\n",
      "loss: 0.0217, acc: 99.6534%\n",
      "CPU times: user 1h 10min 14s, sys: 3min 40s, total: 1h 13min 54s\n",
      "Wall time: 33min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nn_neurons = 64\n",
    "nn_dict = {}\n",
    "nn_counts = 1\n",
    "\n",
    "for nn in nn_models:\n",
    "    \n",
    "    scores = []\n",
    "    abs_errors = []\n",
    "    stage = 0\n",
    "    \n",
    "    print('#'*60)\n",
    "    print('NEURAL NETWORK --> ', nn_counts)\n",
    "    print('#'*60)\n",
    "    \n",
    "    nn_counts += 1\n",
    "    \n",
    "    for train, test in kfold.split(X_scaled):\n",
    "\n",
    "        stage += 1\n",
    "\n",
    "        print('\\nstage --> {}\\n'.format(stage))\n",
    "\n",
    "        X_train_round, X_test_round, y_train_round, y_test_round = X_scaled[train], X_scaled[test], y_scaled[train], y_scaled[test]\n",
    "\n",
    "        nn.fit(X_train_round, y_train_round, batch_size=125, epochs=100, shuffle=True, verbose=0)\n",
    "\n",
    "        # predict\n",
    "        res = nn.predict(X_test_round)\n",
    "        print('R2 of round: ', round(r2_score(y_test_round, res), 4))\n",
    "\n",
    "        # evaluate\n",
    "        loss, acc = nn.evaluate(X_train_round, y_train_round, verbose=1)\n",
    "\n",
    "        reversed_res = scaler_train.inverse_transform(res)\n",
    "        reversed_label = scaler_train.inverse_transform(y_test_round)\n",
    "        abs_errors.append(\n",
    "            {'ACC': acc * 100, 'MAE': mean_abs_error(reversed_res, reversed_label), 'MAX': max(reversed_label), 'MEAN': round(np.mean(reversed_label), 4)}\n",
    "        )\n",
    "\n",
    "        print(\"loss: %.4f, acc: %.4f%%\" % (loss, acc*100))\n",
    "        if acc < 0:\n",
    "            print('X_test round:\\n')\n",
    "            print(X_test_round)\n",
    "            print('#'*60)\n",
    "            print('y_test_round:\\n')\n",
    "            print(y_test_round)\n",
    "            print('#'*60)\n",
    "\n",
    "        scores.append(acc * 100)\n",
    "        \n",
    "    nn_dict['nn_{}'.format(nn_neurons)] = {'MEAN': np.mean(scores), 'STD': np.std(scores) * 100}\n",
    "    nn_neurons *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MEAN            STD       \n",
      "nn_64           99.4807         34.0051   \n",
      "nn_128          99.4154         32.6256   \n",
      "nn_256          99.4201         23.8407   \n",
      "nn_512          99.3731         35.3707   \n",
      "nn_1024         99.4043         29.6436   \n"
     ]
    }
   ],
   "source": [
    "print_dict(nn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_neurons = 64\n",
    "test_eval = dict()\n",
    "\n",
    "for nn in nn_models:\n",
    "    res = nn.predict(X_test_scaled)\n",
    "\n",
    "    scaler_test = StandardScaler().fit(y_test.values)\n",
    "    res = scaler_test.inverse_transform(res)\n",
    "\n",
    "    test_eval['t_{}'.format(nn_neurons)] = {'MAE': mean_abs_error(res, y_test.values), 'MAX': max(y_test.values), 'MEAN': round(np.mean(y_test.values), 2), 'R2': round(r2_score(y_test.values, res) * 100, 4)}\n",
    "\n",
    "    nn_neurons *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork   MAE             MAX             MEAN            R2             \n",
      "t_64            138617.52       13500000.0      1621035.03      98.1044        \n",
      "t_128           136560.85       13500000.0      1621035.03      98.1742        \n",
      "t_256           135910.14       13500000.0      1621035.03      98.149         \n",
      "t_512           135312.39       13500000.0      1621035.03      98.3084        \n",
      "t_1024          142477.57       13500000.0      1621035.03      97.8538        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format('NeuralNetwork', 'MAE', 'MAX', 'MEAN','R2'))\n",
    "for k, v in test_eval.items():\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(k, v['MAE'], v['MAX'][0], v['MEAN'], v['R2'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
