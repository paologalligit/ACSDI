{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from re import sub\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras_tqdm import TQDMNotebookCallback as ktqdm\n",
    "from keras.utils import normalize\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.nn import relu, softmax\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fifa19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0', 'ID', 'Photo', 'Flag', 'Club Logo', 'Real Face', 'Preferred Foot',\n",
    "                 'Body Type', 'Jersey Number', 'Joined', 'Loaned From', 'Contract Valid Until'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obiettivo: predire valore e/o salario dei giocatori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing: convertire value e wage da string a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs=[\"Release Clause\", \"Value\", \"Wage\"]\n",
    "\n",
    "for cur in curs:\n",
    "    def curr2val(x):\n",
    "        x = str(x).replace('â‚¬', '')\n",
    "        if 'K' in x: x = float(str(x).replace('K', '')) * 1000\n",
    "        else: x = float(str(x).replace('M', '')) * 1000000\n",
    "        return x\n",
    "    df[cur] = df[cur].apply(curr2val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scelgo caratteristiche che penso siano sensate per predire il valore di mercato e stipendio del giocatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"LS\", \"ST\", \"RS\", \"LW\", \"LF\", \"CF\", \"RF\", \"RW\",\"LAM\", \"CAM\", \"RAM\", \"LM\", \"LCM\", \"CM\", \"RCM\", \"RM\", \"LWB\", \"LDM\",\"CDM\", \"RDM\", \"RWB\", \"LB\", \"LCB\", \"CB\", \"RCB\", \"RB\"]\n",
    "for col in cols:\n",
    "    df[col] = df[col].str[:-2]\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Height'] = df['Height'].str.replace(\"'\",'.')\n",
    "df['Height'] = df['Height'].astype(float)\n",
    "\n",
    "df['Weight'] = df['Weight'].str[:-3]\n",
    "df['Weight'] = df['Weight'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "\n",
    "# fig = plt.figure(figsize=(50,20))\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(df_corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# ticks = np.arange(0,len(df_corr.columns),1)\n",
    "# ax.set_xticks(ticks)\n",
    "# ax.set_xticklabels(df_corr.columns)\n",
    "# plt.xticks(rotation=90)\n",
    "# ax.set_yticks(ticks)\n",
    "# ax.set_yticklabels(df_corr.columns)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Potential</th>\n",
       "      <th>Value</th>\n",
       "      <th>Wage</th>\n",
       "      <th>International Reputation</th>\n",
       "      <th>Reactions</th>\n",
       "      <th>Release Clause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>110500000.0</td>\n",
       "      <td>565000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>226500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>77000000.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>127100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>118500000.0</td>\n",
       "      <td>290000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>228100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91</td>\n",
       "      <td>93</td>\n",
       "      <td>72000000.0</td>\n",
       "      <td>260000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>138600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>102000000.0</td>\n",
       "      <td>355000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>196400000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Overall  Potential        Value      Wage  International Reputation  \\\n",
       "0       94         94  110500000.0  565000.0                       5.0   \n",
       "1       94         94   77000000.0  405000.0                       5.0   \n",
       "2       92         93  118500000.0  290000.0                       5.0   \n",
       "3       91         93   72000000.0  260000.0                       4.0   \n",
       "4       91         92  102000000.0  355000.0                       4.0   \n",
       "\n",
       "   Reactions  Release Clause  \n",
       "0       95.0     226500000.0  \n",
       "1       96.0     127100000.0  \n",
       "2       94.0     228100000.0  \n",
       "3       90.0     138600000.0  \n",
       "4       91.0     196400000.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "for label in df_corr:\n",
    "#     if df_corr['Value'][label] < 0 or df_corr['Value'][label] > 0.5: labels.append(label)\n",
    "    if df_corr['Value'][label] > 0.5: labels.append(label)\n",
    "        \n",
    "df_flt = df[labels]        \n",
    "df_flt.head()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13655, 4552)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perc = 0.75\n",
    "train_slice = int(len(df_flt) * train_perc)\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "train = df_flt.iloc[:train_slice, :]\n",
    "test = df_flt.iloc[train_slice:, :]\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.loc[:, ['Value']]\n",
    "X_train = train.drop(columns='Value')\n",
    "\n",
    "y_test = test.loc[:, ['Value']]\n",
    "X_test = test.drop(columns='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(y_train)\n",
    "y_train = imputer.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paologio/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# SCALO I VALORI\n",
    "# scaler = RobustScaler()\n",
    "# scaler = scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "\n",
    "# X_train_scaled = preprocessing.scale(X_train)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "# X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_determination(y_test, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square( y_test-y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_test - K.mean(y_test) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_board = TensorBoard(log_dir='value_predictions/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_error(prediction, target):\n",
    "    res = 0\n",
    "    tot = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i][0] > 0 and not np.isnan(prediction[i][0]):\n",
    "            res += abs(target[i][0] - prediction[i][0])\n",
    "            tot += 1\n",
    "#         else: print(target[i], prediction[i][0])\n",
    "\n",
    "    return round(res / tot, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(trainX, trainY,\n",
    "                         hidden_layers,\n",
    "                         num_epochs=10,\n",
    "                         weights_learning_rate=0.5,\n",
    "                         bn_learning_rate=0.5,\n",
    "                         train_batch_size=32,\n",
    "                         momentum_rate=0.9,\n",
    "                         dropout_rate=0.2,\n",
    "                         ini_weights=None,\n",
    "                         ini_biases=None,\n",
    "                         ini_momentums=None,\n",
    "                         ini_gamma=None,\n",
    "                         ini_beta=None):\n",
    "    \n",
    "    trainY = np.array(trainY).reshape(len(trainY), -1)\n",
    "    \n",
    "    layers = hidden_layers + [trainY.shape[1]]\n",
    "    \n",
    "    if ini_weights is None:\n",
    "        weights, biases, momentums, gamma, beta = initialize(layers, trainX.shape[1])\n",
    "    else:\n",
    "        weights, biases, momentums, gamma, beta = ini_weights, ini_biases, ini_momentums, ini_gamma, ini_beta\n",
    "        \n",
    "    trainX_batches, trainY_batches = generate_batches(trainX, trainY, train_batch_size)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # variables used for batch normalization purpose\n",
    "    expected_mean_linear_inp, expected_var_linear_inp = dict(), dict()\n",
    "    exp_mean_linear_inp, exp_var_linear_inp = dict(), dict()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for layer in range(len(layers)):\n",
    "            expected_mean_linear_inp[layer] = np.zeros(weights[layer].shape[1])\n",
    "            expected_var_linear_inp[layer] = np.zeros(weights[layer].shape[1])\n",
    "            \n",
    "        for batch in range(len(trainX_batches)):\n",
    "            trainX_batch = trainX_batches[batch]\n",
    "            trainY_batch = trainY_batches[batch]\n",
    "            \n",
    "            fwd_pass_data = train_forward_pass(trainX_batch, weights, biases, gamma, beta, dropout_rate)\n",
    "\n",
    "            outputs, linear_inp, scaled_linear_inp, mean_linear_inp, var_linear_inp = fwd_pass_data\n",
    "        \n",
    "            for layer in range(len(layers)):\n",
    "                    expected_mean_linear_inp[layer] += mean_linear_inp[layer]\n",
    "                    expected_var_linear_inp[layer] += var_linear_inp[layer]\n",
    "                    \n",
    "            backprop = error_backpropagation(trainX_batch, trainY_batch,\n",
    "                                             outputs=outputs,\n",
    "                                             linear_inp=linear_inp,\n",
    "                                             scaled_linear_inp=scaled_linear_inp,\n",
    "                                             mean_linear_inp=mean_linear_inp,\n",
    "                                             var_linear_inp=var_linear_inp,\n",
    "                                             weights=weights,\n",
    "                                             biases=biases,\n",
    "                                             momentums=momentums,\n",
    "                                             gamma=gamma,\n",
    "                                             beta=beta,\n",
    "                                             bn_learning_rate=bn_learning_rate,\n",
    "                                             weights_learning_rate=weights_learning_rate,\n",
    "                                             momentum_rate=momentum_rate\n",
    "                                            )\n",
    "            \n",
    "            weights, biases, momentums, gamma, beta = backprop\n",
    "            \n",
    "        m = train_batch_size\n",
    "        \n",
    "        for layer in range(len(layers)):\n",
    "            exp_mean_linear_inp[layer] = expected_mean_linear_inp[layer] / len(trainX_batches)\n",
    "            \n",
    "            if m > 1:\n",
    "                exp_var_linear_inp[layer] = (float(m) / (m-1)) * expected_var_linear_inp[layer] / len(trainX_batches)\n",
    "            else:\n",
    "                exp_var_linear_inp[layer] = expected_var_linear_inp[layer] / len(trainX_batches)\n",
    "                \n",
    "        dummy_weights, dummy_biases = scale_weights_dropout(weights, biases, dropout_rate)\n",
    "        \n",
    "        outputs = test_forward_pass(trainX,\n",
    "                                    weights=dummy_weights,\n",
    "                                    biases=dummy_biases,\n",
    "                                    gamma=gamma,\n",
    "                                    beta=beta,\n",
    "                                    mean_linear_inp=exp_mean_linear_inp,\n",
    "                                    var_linear_inp=exp_var_linear_inp\n",
    "                                   )\n",
    "        \n",
    "\n",
    "        curr_loss = loss_reg(outputs, trainY)\n",
    "        cond = len(losses) > 1 and curr_loss > losses[-1] > losses[-2]\n",
    "        \n",
    "        if cond:\n",
    "            weights_learning_rate /= float(2.0)\n",
    "        \n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "    weights, biases = scale_weights_dropout(weights, biases, dropout_rate)\n",
    "    \n",
    "    model = (weights, biases, momentums, gamma, beta, exp_mean_linear_inp, exp_var_linear_inp)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_mean_var(mydata, mean=None, var=None):\n",
    "    if mean is None:\n",
    "        mean = np.mean(mydata, axis=0)\n",
    "    if var is None:\n",
    "        var = np.var(mydata, axis=0)\n",
    "\n",
    "    std_data = (mydata - mean) * (var + 1e-5) ** -0.5\n",
    "\n",
    "    return std_data, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_weights_dropout(weights, biases, dropout_rate):\n",
    "    scaled_weights, scaled_biases = dict(), dict()\n",
    "    \n",
    "    for layer in weights:\n",
    "        scaled_weights[layer] = weights[layer] * (1 - dropout_rate)\n",
    "        scaled_biases[layer] = biases[layer] * (1 - dropout_rate)\n",
    "    \n",
    "    return scaled_weights, scaled_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(layers, num_features):\n",
    "    weights, biases, momentums, gamma, beta = dict(), dict(), dict(), dict(), dict()\n",
    "   \n",
    "    for layer in range(len(layers)):\n",
    "        if layer == 0:\n",
    "            num_rows = num_features\n",
    "            num_cols = layers[layer]\n",
    "        else:\n",
    "            num_rows = layers[layer - 1]\n",
    "            num_cols = layers[layer]\n",
    "        \n",
    "        fan_in = num_rows\n",
    "        \n",
    "        if layer < len(layers)-1:\n",
    "            fan_out = layers[layer + 1]\n",
    "        else:\n",
    "            fan_out = fan_in\n",
    "        \n",
    "        r = 4.0 * math.sqrt(float(6.0) / (fan_in + fan_out))\n",
    "        weights[layer] = np.random.uniform(-r, r, num_rows * num_cols).reshape(num_rows, num_cols)\n",
    "        momentums[layer] = np.zeros((num_rows, num_cols))\n",
    "        biases[layer] = np.zeros(num_cols)\n",
    "        gamma[layer] = np.ones(num_cols)\n",
    "        beta[layer] = np.zeros(num_cols)\n",
    "    \n",
    "    return weights, biases, momentums, gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(trainX, trainY, batch_size):\n",
    "    \n",
    "    concatenated = np.column_stack((trainX, trainY))\n",
    "    np.random.shuffle(concatenated)\n",
    "    trainX = concatenated[:,:trainX.shape[1]]\n",
    "    trainY = concatenated[:,trainX.shape[1]:]\n",
    "    num_batches = math.ceil(float(trainX.shape[0])/batch_size)\n",
    "    \n",
    "    return np.array_split(trainX, num_batches), np.array_split(trainY, num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_pass(trainX, weights, biases, gamma, beta, dropout_rate):\n",
    "    outputs, linear_inp, scaled_linear_inp = dict(), dict(), dict()\n",
    "    mean_linear_inp, var_linear_inp = dict(), dict()\n",
    "    \n",
    "    curr_input = trainX\n",
    "    \n",
    "    for layer in range(len(weights)):\n",
    "        linear_inp[layer] = curr_input.dot(weights[layer]) + biases[layer]\n",
    "        \n",
    "        scaled_linear_inp[layer], mean_linear_inp[layer], var_linear_inp[layer] = standardize_mean_var(\n",
    "            linear_inp[layer])\n",
    "        \n",
    "        shifted_inp = gamma[layer] * scaled_linear_inp[layer] + beta[layer]\n",
    "    \n",
    "        # output layer\n",
    "        if layer == len(weights) - 1:\n",
    "            outputs[layer] = output_layer_activation_reg(shifted_inp)\n",
    "        # hidden layers\n",
    "        else:\n",
    "            binomial_mat = np.zeros(shape=(trainX.shape[0], weights[layer].shape[1]))\n",
    "            for row in range(trainX.shape[0]):\n",
    "                binomial_mat[row,] = np.random.binomial(1, 1 - dropout_rate, weights[layer].shape[1])\n",
    "            \n",
    "            outputs[layer] = hidden_layer_activation_relu(shifted_inp) * binomial_mat\n",
    "            \n",
    "        curr_input = outputs[layer]\n",
    "    \n",
    "    return outputs, linear_inp, scaled_linear_inp, mean_linear_inp, var_linear_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_backpropagation(trainX, trainY,\n",
    "                          outputs,\n",
    "                          linear_inp, scaled_linear_inp,\n",
    "                          mean_linear_inp, var_linear_inp,\n",
    "                          weights, biases, momentums, gamma, beta,\n",
    "                          bn_learning_rate, weights_learning_rate, momentum_rate\n",
    "                         ):\n",
    "    \n",
    "    bp_grads_1, bp_grads_2 = dict(), dict()\n",
    "    inverse_num_examples = float(1.0) / trainX.shape[0]\n",
    "    \n",
    "    for layer in reversed(range(len(weights))):\n",
    "        denom = (var_linear_inp[layer] + 1e-5) ** -0.5\n",
    "        numer = linear_inp[layer] - mean_linear_inp[layer]\n",
    "    \n",
    "        # input layer\n",
    "        if layer == len(weights) - 1:\n",
    "            bp_grads_2[layer] = output_layer_grad_reg(outputs[layer], trainY)\n",
    "        # output and hidden layers\n",
    "        else:\n",
    "            bp_grads_2[layer] = hidden_layer_grad_relu(outputs[layer])\n",
    "            next_layer_weights = weights[layer + 1]\n",
    "            bp_grads_2[layer] *= bp_grads_1[layer + 1].dot(next_layer_weights.T)\n",
    "        \n",
    "        a = bp_grads_2[layer] * gamma[layer]\n",
    "        b = np.sum(a * (-0.5 * (denom ** 3.0)) * numer, axis=0)\n",
    "        c = np.sum(-a * denom, axis=0) + b * np.sum(-2.0 * numer) * inverse_num_examples\n",
    "        bp_grads_1[layer] = a * denom + b * 2.0 * numer * inverse_num_examples + c * inverse_num_examples\n",
    "        \n",
    "        if layer > 0:\n",
    "            total_err = outputs[layer - 1].T.dot(bp_grads_1[layer])\n",
    "        else:\n",
    "            total_err = trainX.T.dot(bp_grads_1[layer])\n",
    "        \n",
    "        beta[layer] -= bn_learning_rate * np.sum(bp_grads_2[layer], axis=0) * inverse_num_examples\n",
    "        gamma[layer] -= bn_learning_rate * np.sum(bp_grads_2[layer] * scaled_linear_inp[layer],\n",
    "                                                    axis=0) * inverse_num_examples\n",
    "        momentums[layer] = momentum_rate * momentums[layer] - weights_learning_rate * total_err * inverse_num_examples\n",
    "        weights[layer] += momentums[layer]\n",
    "        biases[layer] -= weights_learning_rate * np.sum(bp_grads_1[layer], axis=0) * inverse_num_examples\n",
    "    \n",
    "    return weights, biases, momentums, gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(testX, weights, biases, gamma, beta, mean_linear_inp, var_linear_inp):\n",
    "    outputs = dict()\n",
    "    curr_input = testX\n",
    "    for layer in range(len(weights)):\n",
    "        linear_inp = curr_input.dot(weights[layer]) + biases[layer]\n",
    "        scaled_linear_inp, _, _ = standardize_mean_var(linear_inp, mean=mean_linear_inp[layer],\n",
    "                                                       var=var_linear_inp[layer])\n",
    "        shifted_inp = gamma[layer] * scaled_linear_inp + beta[layer]\n",
    "        if layer == len(weights) - 1:\n",
    "            outputs[layer] = output_layer_activation_reg(shifted_inp)\n",
    "        else:\n",
    "            outputs[layer] = hidden_layer_activation_relu(shifted_inp)\n",
    "        curr_input = outputs[layer]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(preds, actuals):\n",
    "    return np.sum(np.sum(-actuals * np.log2(preds), axis=0)) / preds.shape[0]\n",
    "\n",
    "def loss_mse(preds, actuals):\n",
    "    return np.sum(np.sum(0.5 * (preds - actuals) ** 2, axis=0)) / preds.shape[0]\n",
    "\n",
    "def loss_class(outputs, targets):\n",
    "    num_layers = len(outputs)\n",
    "    predictions = outputs[num_layers - 1]\n",
    "    total_loss = loss_cross_entropy(predictions, targets)\n",
    "    return total_loss\n",
    "\n",
    "def loss_reg(outputs, targets):\n",
    "    num_layers = len(outputs)\n",
    "    predictions = outputs[num_layers - 1]\n",
    "    total_loss = loss_mse(predictions, targets)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_activation_sigmoid(inputs):\n",
    "    return (1.0 + np.exp(-inputs))**-1.0\n",
    "\n",
    "def hidden_layer_grad_sigmoid(inputs):\n",
    "    return inputs * (1 - inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_activation_relu(inputs):\n",
    "    return np.maximum(0.1 * inputs, 0.9 * inputs)\n",
    "\n",
    "def hidden_layer_grad_relu(inputs):\n",
    "    temp = inputs\n",
    "    temp[temp <= 0.0] = 0.1\n",
    "    temp[temp > 0.0] = 0.9\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer_activation_reg(inputs):\n",
    "    return inputs\n",
    "\n",
    "def output_layer_grad_reg(pred_outs, true_outs):\n",
    "    return pred_outs - true_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_neural_network(testX, model):\n",
    "    \n",
    "    weights, biases, _, gamma, beta, exp_mean_linear_inp, exp_var_linear_inp = model\n",
    "    num_layers = len(weights)\n",
    "    \n",
    "    outputs = test_forward_pass(testX,\n",
    "                                weights=weights,\n",
    "                                biases=biases,\n",
    "                                gamma=gamma,\n",
    "                                beta=beta,\n",
    "                                mean_linear_inp=exp_mean_linear_inp,\n",
    "                                var_linear_inp=exp_var_linear_inp)\n",
    "    \n",
    "    preds = outputs[num_layers - 1]\n",
    "    outs = []\n",
    "    \n",
    "#     print('preds {} and preds.shape[0]: {}'.format(preds, preds.shape[0]))\n",
    "    for value in preds.values:\n",
    "        outs += [value]\n",
    "            \n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(trainX, hidden_layers, num_epochs,\n",
    "                      weights_learning_rate, bn_learning_rate, momentum_rate, dropout_rate,\n",
    "                      ini_weights, ini_biases, ini_momentums, ini_gamma, ini_beta):\n",
    "    layers = hidden_layers\n",
    "    weights, biases, momentums, gamma, beta = ini_weights, ini_biases, ini_momentums, ini_gamma, ini_beta\n",
    "    exp_mean_linear_inp, exp_var_linear_inp = dict(), dict()\n",
    "    curr_input = trainX\n",
    "    for layer in range(len(hidden_layers)):\n",
    "        l_weights, l_biases, l_momentums, l_gamma, l_beta = initialize([layers[layer], curr_input.shape[1]],\n",
    "                                                                       curr_input.shape[1])\n",
    "        l_weights[0], l_biases[0], l_momentums[0], l_gamma[0], l_beta[0] = ini_weights[layer], ini_biases[layer], \\\n",
    "                                                                           ini_momentums[layer], ini_gamma[layer], \\\n",
    "                                                                           ini_beta[layer]\n",
    "        model = train_neural_network(curr_input, curr_input,\n",
    "                                     hidden_layers=[layers[layer]],\n",
    "                                     num_epochs=num_epochs,\n",
    "                                     weights_learning_rate=weights_learning_rate,\n",
    "                                     bn_learning_rate=bn_learning_rate,\n",
    "                                     train_batch_size=trainX.shape[0],\n",
    "                                     momentum_rate=momentum_rate,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     ini_weights=l_weights,\n",
    "                                     ini_biases=l_biases,\n",
    "                                     ini_momentums=l_momentums,\n",
    "                                     ini_gamma=l_gamma,\n",
    "                                     ini_beta=l_beta,\n",
    "                                     type=\"regression\")\n",
    "        m_weights, m_biases, m_momentums, m_gamma, m_beta, m_exp_mean_linear_inp, m_exp_var_linear_inp = model\n",
    "        weights[layer], biases[layer], momentums[layer], gamma[layer], beta[layer] = m_weights[0], m_biases[0], \\\n",
    "                                                                                     m_momentums[0], m_gamma[0], m_beta[\n",
    "                                                                                         0]\n",
    "        exp_mean_linear_inp[layer], exp_var_linear_inp[layer] = m_exp_mean_linear_inp[0], m_exp_var_linear_inp[0]\n",
    "        outputs = test_forward_pass(curr_input,\n",
    "                                    weights=m_weights,\n",
    "                                    biases=m_biases,\n",
    "                                    gamma=m_gamma,\n",
    "                                    beta=m_beta,\n",
    "                                    mean_linear_inp=m_exp_mean_linear_inp,\n",
    "                                    var_linear_inp=m_exp_var_linear_inp,\n",
    "                                    type=\"regression\")\n",
    "        curr_input = outputs[0]\n",
    "    return weights, biases, momentums, gamma, beta, exp_mean_linear_inp, exp_var_linear_inp, curr_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_neural_network(X_train, y_train, [300, 256, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict_neural_network(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = X_train.shape[1]\n",
    "N_CLASSES = 1\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, neurons, batchsize, stop_function, stop_parameter):\n",
    "        self.input_size = N_FEATURES # dimensione immagine\n",
    "        self.output_size = N_CLASSES # i 10 numeri da riconoscere\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.stop_f = stop_function # 2\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # Standardize random weights\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons # matrice neurons * input_size\n",
    "        output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "        self.layers = [hidden_layer, output_layer]\n",
    "\n",
    "    def train(self, X_training, y_training, X_testing, y_testing):\n",
    "        \n",
    "        accu_train = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(X_training.shape[0])\n",
    "        len_batch_test = len(X_testing.shape[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "#         test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "#         test_input = training[0:len_batch_train][0:len_batch_train]\n",
    "#         inputs = training[0][0:len_batch_train]\n",
    "#         targets = np.zeros((len_batch_train, 10))\n",
    "#         for i in range(len_batch_train):\n",
    "#             targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu_train[1]):\n",
    "            \n",
    "            self.iteration += 1\n",
    "            \n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                print('input_vec --> {}\\ntarget_vect  --> {}\\n'.(input_vector, target))\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu_test = self.accu(X_testing, y_testing)\n",
    "#             accu_train = self.accu(test_input)\n",
    "            \n",
    "            # Messages\n",
    "            if (self.iteration == 1 or self.iteration % 10 == 0):\n",
    "                self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "                \n",
    "        # Print last epoch\n",
    "        if (self.iteration % 10 != 0):\n",
    "            self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(layer, input_with_bias)\n",
    "            output = special.expit(output) # expit is the sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        c = 10**(-4) + 10**(-1)/math.sqrt(self.iteration)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        self.layers[-1] -= c*np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(self.layers[-1], self.neurons, 1).T, output_deltas)\n",
    "        self.layers[0] -= c*np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing_batch):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing_batch[1])):\n",
    "            if self.predict_one(testing_batch[0][k]) == testing_batch[1][k]:\n",
    "                res[testing_batch[1][k]] += 1\n",
    "            else:\n",
    "                res[testing_batch[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu_test,accu_train,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy TRAIN: '+str(accu_train[1]).zfill(4)+'%\\t'\n",
    "        message += 'Accuracy TEST: '+str(accu_test[1]).zfill(4)+'%\\t'\n",
    "        message += 'Min: '+ str(accu_test[0]).zfill(4)+ '% ('+str(int(accu_test[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(neurons=300, batchsize=250, stop_function=2, stop_parameter=0.01)\n",
    "# nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
